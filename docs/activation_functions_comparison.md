# Сравнительный анализ функций активации в SAC-проекте

| Название         | Формула (LaTeX)                                                                 | Свойства и плюсы                         | Научная роль в RL/NN                | Где обычно применяется              |
|------------------|---------------------------------------------------------------------------------|------------------------------------------|-------------------------------------|-------------------------------------|
| **ReLU**         | $f(x) = \max(0, x)$                                                             | Быстрая, не насыщается для $x>0$, разреженная активация | Ускоряет обучение, предотвращает затухание градиентов | Скрытые слои MLP/LSTM              |
| **LeakyReLU**    | $f(x) = \begin{cases} x, & x \geq 0 \\ \alpha x, & x < 0 \end{cases}$       | Нет "мёртвых" нейронов, простая          | Сохраняет градиент для отрицательных значений | Скрытые слои, часто в RL           |
| **ELU**          | $f(x) = \begin{cases} x, & x \geq 0 \\ \alpha (e^x - 1), & x < 0 \end{cases}$ | Сглаживает отрицательные значения, быстрее сходимость | Улучшает обучение, уменьшает смещение | Скрытые слои, иногда выходы критика |
| **SELU**         | $f(x) = \lambda \begin{cases} x, & x \geq 0 \\ \alpha (e^x - 1), & x < 0 \end{cases}$ | Самонормализующаяся сеть, автоматическая стабилизация | Поддерживает нормализацию активаций | Глубокие сети, редко в RL           |
| **Tanh**         | $f(x) = \tanh(x)$                                                               | Ограничивает выход $[-1, 1]$, центрирован | Для выходных слоёв политики (актор) | Выходные слои, иногда скрытые       |
| **Sigmoid**      | $f(x) = \frac{1}{1 + e^{-x}}$                                                   | Ограничивает выход $[0, 1]$            | Для вероятностных выходов           | Выходные слои, бинарные задачи      |
| **Swish**        | $f(x) = x \cdot \sigma(x)$, где $\sigma(x) = \frac{1}{1 + e^{-x}}$           | Гладкая, неограниченная, улучшает обучение | Улучшает обобщение, гибкая          | Современные глубокие сети           |
| **Mish**         | $f(x) = x \cdot \tanh(\ln(1 + e^x))$                                          | Очень гладкая, сильное обобщение         | Улучшает эксплорацию, smoothness     | Современные RL/vision сети          |
| **GELU**         | $f(x) = x \cdot \Phi(x)$, где $\Phi(x)$ — CDF нормального распределения       | Гладкая, стохастическая, лучше ReLU      | Улучшает обучение, часто в NLP       | Transformers, современные RL        |
| **FusedSiLU**    | $f(x) = x \cdot \sigma(x)$, где $\sigma(x) = \frac{1}{1 + e^{-x}}$           | Быстрее Swish на CUDA                    | Как Swish, но быстрее               | Современные CUDA-сети               |
| **FastGELU**     | $f(x) = x \cdot \sigma(1.702x)$, где $\sigma(x) = \frac{1}{1 + e^{-x}}$      | Быстрее GELU, почти те же свойства       | Быстрое обучение, smoothness         | Transformers, RL                    |
| **HardSwish**    | $f(x) = x \cdot \frac{\mathrm{ReLU6}(x + 3)}{6}$, где $\mathrm{ReLU6}(x) = \min(\max(0, x), 6)$ | Быстрая, аппроксимация Swish             | Быстрое обучение, экономия ресурсов  | Mobile/embedded, RL                 |

</rewritten_file> 